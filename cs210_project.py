# -*- coding: utf-8 -*-
"""CS210 Proje - Group 38

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eKoCcg-TFEWPEkI2bRjb1mIvkrAj4NkL

# **Spotify Basic Recommendation System Project**

Group 38 Members:

Ali Tarık Mırık \
Kaan Atmaca \
Melis Kudat

## Introduction

Spotify is one of the most popular music platforms that enables people to enjoy listening to music depending on their frame of mind and taste. By providing the users a large scale of songs that have different features such as danceability, acousticness, popularity, energy... In this platform, in an attempt to please the users, songs are being offered by Spotify according to the properties of the previous choices of the users. In our project, we examined more than one hundred sixty thousand (+160.000) data involving features of the songs and compared those features with each other in order to catch resemblances and make analysis according to those resemblances. Furthermore, we analyzed the most popular artists and songs in the stated years. In the following steps, the effect of the correlations of the features on the popularity of the songs will be analyzed by statistical tests and the alteration of the preference of the music genres by the time will be demonstrated. As a last step, according to our data analysis and observations, we will create a song recommendation system and predict the popularity of the upcoming songs.

## **Exploratory Data Analysis**

The datasets that we have examined involve the features of the songs on Spotify such as duration, energy, and danceability. In order to analyze the data and make connections, we used 5 datasets. Each dataset contains different features in different numbers. As an instance of this situation data_by_year.csv and data.csv can be given. The dataset of data_by_year involves 14 features about each year while data.csv contains 19 features of songs. The same situation can be observed in the other datasets as well. In order to make our analysis more relatable and multifarious we made use of the different datasets that were given to us. In this step, by using those datasets, we visualized our data by drawing scatters, matrices, and histograms in order to understand our data in a more preferable way. Besides, we analyzed the most popular songs and artists, and we researched about how the genres and features changed overtime.

### Loading Modules and Data
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from scipy import stats
from scipy import special

from google.colab import drive
drive.mount('/content/drive')

df_main = pd.read_csv("/content/drive/My Drive/CS210/data.csv")
df_artist = pd.read_csv("/content/drive/My Drive/CS210/data_w_genres.csv")
df_genre = pd.read_csv("/content/drive/My Drive/CS210/data_by_genres.csv")
df_year = pd.read_csv("/content/drive/My Drive/CS210/data_by_year.csv")

"""### Description of the Dataset & Preprocessing

*We started by bringing the first 5 rows of our data into the forefront to visualize a small part of our data*
"""

df_main.head()

df_main.tail()

"""*We also printed the statistical properties of the feautes to take a general look to our data.*

"""

df_main.describe()

print("Number of rows (samples) are:", df_main.shape[0], "rows")
print("Number of columns (features) are:", df_main.shape[1], "columns")
df_main.shape

print("Feature types and counts:")
df_main.info()

"""#### Manipulating & Cleaning The Data

*We have over 160.000 rows in our data and there might be invalid situations such as null or unclear data. Also we wanted to add columns (from other datasets to our main dataset) to do so we merged and manipulated our datasets.*

*Firstly, we checked for null values in our data. As a result the data was clean and everything was properly assigned.*
"""

df_main.isna()

"""*In the main dataset the "Artists" strings were stored with brackets and apostrophes. So we cleaned the strings then stored the cleaned strings in another column called "Artist".*"""

def clean_artist_name(artist_name):
  artist_name = artist_name.replace('[', '')
  artist_name = artist_name.replace(']', '')
  artist_name = artist_name.replace('"', '')
  artist_name = artist_name.replace('\'','')
  return artist_name

df_main["artist"] = df_main["artists"].apply(clean_artist_name)

"""*We had dataframes for songs, artists, genres and years in different datasets. However, we needed the genre of songs to be in the same dataset with the main data set which also had the feature of years. To do so, we merged our df_artist and df_main to be able to observe and analyze the relationship between genres,years, and popularity.*"""

def assign_genre(artist_name):
  mask =  df_artist["artists"].values == artist_name
  df_new = df_artist.loc[mask]
  myList = df_new["genres"].tolist()
  return str(myList).replace('[','').replace(']','').replace('"','')

df_main["genres"] = df_main["artist"].apply(assign_genre)

df_main.head(5)[["artist", "name", "genres", "year"]]

"""###  Visualizations Descriptive Statistics

*These histograms demonstrate the total counts of features (in the ranges of minimum and maximum values of each feature).*
"""

plt.figure(figsize=(44,4))
attrs = ["acousticness", "danceability", "energy", "instrumentalness", "liveness", "loudness", "speechiness", "tempo", "valence"]
colors = ["darkslategray", "lightslategray", "black", "black", "darkslategray", "lightslategray", "lightslategray", "black", "darkslategray"]
for i in range(len(attrs)):
  plt.grid(axis="y")
  plt.subplot(1, len(attrs), i+1)
  sns.histplot(df_main[attrs[i]], bins=10, color = colors[i])
  if i == 2 or i == 5:
    plt.tight_layout()
    plt.grid(axis="y")
    plt.show()
    plt.figure(figsize=(43.15,4))
  
plt.tight_layout()
plt.grid(axis="y")
plt.show()

"""*This bar chart provides us the information about the means of the features of the songs which are released in the years between 1920 to 2021.*"""

acc = df_main["acousticness"].mean()
liv = df_main["liveness"].mean()
ene = df_main["energy"].mean()
dan = df_main["danceability"].mean()
spe = df_main["speechiness"].mean()
val = df_main["valence"].mean()

name_of_att = ["danceability", "valence", "acousticness", "energy","liveness",  "speechiness" ]
att = [ dan, val, acc, ene,liv,  spe]
plt.figure(figsize=(11,5)) 
plt.bar(name_of_att, att, color = 'paleturquoise', edgecolor="black")
  
plt.xlabel('Attributes')
plt.ylabel('Mean')
plt.title("Means of Attributes")
plt.grid(axis="y")
plt.show()

"""*This bar chart provides us the information about the standart derivation of the features of the songs which are released in the years between 1920 to 2021.*"""

acc = df_main["acousticness"].std()
liv = df_main["liveness"].std()
ene = df_main["energy"].std()
dan = df_main["danceability"].std()
spe = df_main["speechiness"].std()
val = df_main["valence"].std()

name_of_att = ["danceability", "valence", "acousticness", "energy","liveness",  "speechiness" ]
att = [ dan, val, acc, ene,liv,  spe]
plt.figure(figsize=(11,5)) 
plt.bar(name_of_att, att, color = 'teal', edgecolor="black")
  
plt.xlabel('Attributes')
plt.ylabel('Std')
plt.title("Standart Derivation of Attributes")
plt.grid(axis="y")
plt.show()

"""*The following four scatter plots show the correlations of popularity with the features acousticness, liveness, energy, and danceability. Doing so we tried to find the correlations of the features with popularity.*"""

plt.figure(figsize=(10,3))
sample_data = df_main.sample(100000)
plt.title("Popularity Correlaiton")
plt.scatter(sample_data["acousticness"], sample_data["popularity"], marker='.', color='paleturquoise', label='ys1')
plt.xlabel("acousticness")
plt.ylabel("popularity")
plt.show()

plt.figure(figsize=(10,3))
plt.scatter(sample_data["liveness"], sample_data["popularity"], marker='.', color='darkslategray', label='ys2')
plt.xlabel("liveness")
plt.ylabel("popularity")
plt.show()

plt.figure(figsize=(10,3))
plt.scatter(sample_data["energy"], sample_data["popularity"], marker='.', color='teal', label='ys2')
plt.xlabel("energy")
plt.ylabel("popularity")
plt.show()

plt.figure(figsize=(10,3))
plt.scatter(sample_data["danceability"], sample_data["popularity"], marker='.', color='lightslategray', label='ys2')
plt.xlabel("danceability")
plt.ylabel("popularity")
plt.show()

"""*This scatter plot demonstrates the correlation between loudness and energy, with this scatter plot we can observe that there is a positive correlation between those two features.*"""

plt.figure(figsize=(15,4))
plt.scatter(df_main["energy"], df_main["loudness"], marker='.', color='darkslategray')
plt.xlabel("energy")
plt.ylabel("loudness")
plt.show()

"""*This plot shows the negative correlation between energy and acousticness.*"""

plt.figure(figsize=(15,4))
plt.scatter(df_main["energy"].head(50000), df_main["acousticness"].head(50000), marker='.', color='darkslategray', label='ys2')
plt.xlabel("energy")
plt.ylabel("acousticness")
plt.show()

"""*This correlation matrix demonstrates us the the correlations between all features that provides us to examine whether the correlation of two features are negative, positive or whether they are correlated or not.*"""

plt.figure(figsize=(15,12))
palette = sns.diverging_palette(20, 240, n=100)
corr = df_main.corr()
sns.heatmap(corr, annot=True, fmt=".2f",  cmap=palette, square=True, center = 0)
plt.title("Correlation Matrix",size=15, weight='bold')
plt.show()

"""### Analysis of the Most Popular Artists and Songs

*The following horizontal bar chart provides us the top 10 songs that have the highest popularities between the years of 1920 and 2021*
"""

values = df_main.sort_values(by=["popularity"]).tail(10)["popularity"].to_list()
categories = df_main.sort_values(by=["popularity"]).tail(10)["name"].to_list()
plt.figure(figsize=(14,8))
plt.barh(categories, values, color = "lightslategray", edgecolor="black")

for value, cat in zip(values, categories):
  plt.text(value, cat, f"{value:.2f}", verticalalignment="center", fontsize=10)

plt.xlim([0, max(values)+10])
plt.xlabel("Popularities")
plt.ylabel("Songs")
plt.title("Top Songs of All Time ")
plt.grid(axis="x")

plt.show()

"""*The following horizontal bar chart demonstrates the most popular artists in the year range of 1920-2021.*"""

values = df_main.sort_values(by=["popularity"]).tail(10)["popularity"].to_list()
artists = df_main.sort_values(by=["popularity"]).tail(10)["artist"].to_list()
plt.figure(figsize=(14,8))
plt.barh(artists, values, color = "gray", edgecolor="black")

for value, cat in zip(values, artists):
  plt.text(value, cat, f"{value:.2f}", verticalalignment="center", fontsize=10)

plt.xlim([0, max(values)+10])
plt.xlabel("Popularities")
plt.ylabel("Songs")
plt.title("Top Artists of All Times")
plt.grid(axis="x")
plt.show()

"""*In this part of the code, the year is asked to the user as an input. After the input is taken, the table gives the user information of the top 5 songs in the years between (input year - 5) and (input year + 5 )*"""

year = int(input("Enter a year between 1920-2021: "))
while(year < 1920 or year > 2021):
  year = input("Wrong input. Please enter a year between 1920-2021: ")
df_filtered = df_main[df_main["year"] < 5 + year]
df_filtered = df_filtered[df_filtered["year"] > year - 5]
df_filtered = df_filtered.sort_values(by=['popularity'], ascending=False)

print("Top five songs of", year-5, "-", year+5, ":")
df_filtered[["name", "artists", "popularity", "release_date"]].head(5)

"""*In these bar charts we wanted to compare the means of the features of the most popular 50 songs and all the songs.*"""

top_50 = df_main.sort_values(by=["popularity"]).tail(50)
attrs = ["acousticness", "danceability", "energy", "instrumentalness", "liveness", "speechiness", "tempo", "valence"]
plot_data = {x:[top_50[x].mean(), df_main[x].mean()] for x in attrs}
fig, axs = plt.subplots(1,8, figsize=(25,6))
colors = ["darkslategray", "slategray"]

for i in range(8):
  a = axs[i].bar(["TOP50","ALL"],plot_data[attrs[i]], edgecolor="black")
  a[0].set_color(colors[0])
  a[1].set_color(colors[1])
  axs[i].set_title(attrs[i], fontsize=14)
  axs[i].grid(axis="y")

plt.tight_layout()
plt.show()

"""### Genres & Features Over Time

*The following horizontal bar chart provides us the top 10 most used genres in the years between 1920-2021*
"""

def get_mostused_genres():
  genre_popularities = {}
  for idx, song in df_main.iterrows():
    if len(song["genres"])!=0:
      genres = song["genres"].strip("[]'").split("', '")
      for g in genres:
        if g not in genre_popularities.keys():
          genre_popularities[g] = 1
        else:
          genre_popularities[g] += 1
  
  top_genres = list(sorted(genre_popularities.items(), key=lambda item: item[1]))
  top_genres.reverse() 
  for i in top_genres: 
    if i[0] == "" : 
      top_genres.remove(i)
  top_genres = top_genres[:10]
  return top_genres

top_genres=get_mostused_genres()
plt.figure(figsize=(14,8))
for i in top_genres:
  plt.bar(i[0], i[1], color="teal",  edgecolor="black")

plt.xlabel("Genres")
plt.ylabel("Song count")
plt.title("Most Used Genres in Songs")
plt.grid(axis="y")
plt.show()

"""*The following horizontal bar chart provides us the top 10 genres that have the highest popularities.*"""

values = df_genre.sort_values(by=["popularity"]).tail(10)["popularity"].to_list()
categories = df_genre.sort_values(by=["popularity"]).tail(10)["genres"].to_list()
plt.figure(figsize=(14,8))
plt.barh(categories, values, color="paleturquoise", edgecolor="black")

for value, cat in zip(values, categories):
  plt.text(value, cat, f"{value:.2f}", verticalalignment="center", fontsize=10)

plt.xlim([0, max(values)+10])
plt.xlabel("Popularities")
plt.ylabel("Genres")
plt.title("Top Genres of All Time")
plt.grid(axis="x")
plt.show()

"""*To analyze the genres change over time, we counted the number of songs in that genre in a range of years. We choose to analyze for every 10 years. There can be seen that some genres gain popularity over time and some become less popular. Pop songs for example, had a spike in popularity in recent years.*"""

def get_popular_genres(year):
  df_top_songs = df_main[df_main["year"] < 5 + year]
  df_top_songs = df_top_songs[df_top_songs["year"] > year - 5]
  df_top_songs = df_top_songs.sort_values(by=['popularity'], ascending=False)
  df_top_songs = df_top_songs.head(100)

  genre_popularities = {}

  for idx, song in df_top_songs.iterrows():
    if len(song["genres"])!=0:
      genres = song["genres"].strip("[]'").split("', '")
      for g in genres:
        if g not in genre_popularities.keys():
          genre_popularities[g] = 1
        else:
          genre_popularities[g] += 1
  top_genres = list(sorted(genre_popularities.items(), key=lambda item: item[1]))
  top_genres.reverse()
  top_genres = top_genres[:4]

  return top_genres

colors = ["darkslategray", "lightslategray", "black", "black", "darkslategray", "lightslategray", "lightslategray", "black", "darkslategray", "black"]
fig, axs = plt.subplots(4,3,figsize=(20,17), gridspec_kw={'hspace': 0.4, 'wspace': 0.2})
for i in range(10):
  top_genres=get_popular_genres(1925 + i*10)
  genres = [x[0] for x in top_genres]
  counts = [x[1] for x in top_genres]
  axs[int(i/3), i%3].bar(genres, counts, color = colors[i], edgecolor="black")
  axs[int(i/3), i%3].set_title("Top Genres by Number of Songs of " + str(1920 + i*10) + "'s")
  axs[int(i/3), i%3].set_xlabel("Genres")
  axs[int(i/3), i%3].set_ylabel("Counts")
  axs[int(i/3), i%3].grid(axis="y")

fig.delaxes(axs[3][1])
fig.delaxes(axs[3][2])

plt.show()

"""*The lines in the plot demonstrate the increase or the decrease in the specified features of the songs in the time range of 1920-2021. According to the results, we can claim that zigzags are observed in all of the three features between 1920- 1950. After that year, liveness of the songs show stability while the other two properties demonstrate sharp increase and decrease.*"""

plt.figure(figsize=(15,9))
plt.plot(df_main.groupby('year').mean()['energy'], label="energy", color = "mediumaquamarine")
plt.plot(df_main.groupby('year').mean()['acousticness'], label="acousticness", color = "slategray")
plt.plot(df_main.groupby('year').mean()['liveness'], label="liveness", color = "aqua")
plt.legend()
plt.title("Features Related to Years",size=15, weight='bold')
plt.grid()
plt.show()

"""*In this part, we observed the state of affairs of two features that are instrumenalness and valence. By that, we can claim that for both of the two features, statibility is not observed while sharp increase and decrease are seen between the years of 1920-2020*"""

plt.figure(figsize=(15,9))
plt.plot(df_year.groupby('year').mean()['instrumentalness'], label="instrumentalness", color = "gray")
plt.plot(df_year.groupby('year').mean()['valence'], label="valence", color = "aqua")
plt.legend()
plt.title("Features Related to Years",size=15, weight='bold')
plt.grid()
plt.show()

"""As we have observed by analyzing and visualizing our data, the features of the songs have significant influence on the popularity of the songs on Spotify. Therefore, understanding how and in which range those features affect the popularity has a huge importance for the prediction of the popularity of the upcoming songs. In the next steps of our project, we will try to examine the contributions of the features on the popularity by applying statistical tests. Later on, we will evaluate the alteration of the features in a different timeline for the same music genres by statistical tests as well. As we stated before, the prediction of the songs’ popularity is extremely important to increase the preferability of the songs. Therefore, we will use several machine learning models to predict the popularity. In order to increase the productivity of the models, we will make endeavor on hyper-parameter tuning. As our last step, we will create our own song recommendation system by using similarities and methods such as Nearest Neighbors.

## Hyphothesis Testing

### Hypothesis 1


Hypothesis Test: Our purpose in this part is to examine whether or not there exists an obvious change in the popularity of the Spotify songs depending on the mode (minor or major).


Null Hypothesis ($H_0$): Means of `popularity` samples for all modes of the songs (minor or major) have the same value (e.g. $m_1$ denotes 1st mode of the sample songs)

$ {H_0}: \mu{m1} = \mu{m_2} $

Alternative Hypothesis ($H_A$): Means of `popularity` samples for all modes of the songs are not the same.


$ {H_A}:$ Means $\mu{m1}, \mu{m_2} $ are not same.

Significance level: As most of hypothesis tests assume significance level as 0.05, we are setting it as 0.05 for our test too.
"""

sample_1 = df_main[df_main['mode'] == 0]
sample_2 = df_main[df_main['mode'] == 1]

fig, ax = plt.subplots(1, 3, figsize=(14,5))
        
sample_1['popularity'].plot(kind="hist", ax=ax[0], bins=20, label="completed", color="darkslategray", density=True)
ax[0].set_title("Mode: 0 (Minor)")

sample_2['popularity'].plot(kind="hist", ax=ax[1], bins=20, label="none", color="slategray", density=True)
ax[1].set_title("Mode: 1 (Major)")

sns.kdeplot(sample_1['popularity'], shade=True, label="Target: 0", ax=ax[2], color="darkslategray")
sns.kdeplot(sample_2['popularity'], shade=True, label="Target: 1", ax=ax[2], color="slategray")
ax[2].set_title("Comparison with KDE")

plt.suptitle("Popularity Distributions")

# To make a clearer layout for the graphs
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

f_stats, p_values = stats.f_oneway(sample_1['popularity'].values, sample_2['popularity'].values)
p_values

"""A p-value higher than 0.05 has the meaning of supporting the null hypothesis and rejecting the alternative hypothesis. In our case, we had the value of 0.001 that led us to understand that we had to reject null hypothesis and accept the alternative hypothesis. Which means popularity changes with the mode of the song (i.e. whether the song is in minor or major cord).

### Hypothesis 2

Hypothesis Test: Our purpose in this part is to examine the change in the features of the pop songs before and after the millenium (2000).


Null Hypothesis ($H_0$): Means of `danceability`, `popularity` and `acousticness` samples for songs before the year 2000 have the same value with the means of the features of the songs that came after 2000 (e.g. $md_1$ denotes mean of danceability for songs before 2000).

$ {H_0}: \mu{md_1} = \mu{md_2} \ \ \mu{mp_1} = \mu{mp_2} \ \ \mu{ma_1} = \mu{ma_2}$

Alternative Hypothesis ($H_A$): Means of `danceability`, `popularity` and `acousticness` samples changed before and after the year 2000.

$ {H_A}:$ Means $\mu{md_1}, \mu{md_2} $ and $ \mu{mp_1}, \mu{mp_2} $ and $ \mu{ma_1}, \mu{ma_2} $ are not same.

Significance level: We are setting it as 0.05 for our this test too.
"""

# Creating samples based on the year and the genre

sample_1 = df_main[df_main['year'] < 2000]
sample_1 = sample_1[sample_1['genres'].str.contains("pop")]

sample_2 = df_main[df_main['year'] >= 2000]
sample_2 = sample_2[sample_2['genres'].str.contains("pop")]

#we want to examine this three features
features = ['danceability', 'popularity', 'acousticness']

fig, ax = plt.subplots(3, 3, figsize=(13,13))

for i in range(len(features)):
  f = features[i]
  sample_1[f].plot(kind="hist", ax=ax[i][0], bins=20, label="completed", color="teal", density=True)
  ax[i][0].set_title("Pop Songs Before 2000")

  sample_2[f].plot(kind="hist", ax=ax[i][1], bins=20, label="none", color="darkslategray", density=True)
  ax[i][1].set_title("Pop Songs After 2000")

  sns.kdeplot(sample_1[f], shade=True, label="Target: 0", ax=ax[i][2], color="teal")
  sns.kdeplot(sample_2[f], shade=True, label="Target: 1", ax=ax[i][2], color="darkslategray")
  ax[i][2].set_title("Comparison with KDE")

  print(stats.ttest_ind(sample_1[f], sample_2[f], equal_var=False))  # since we have not equal variances


plt.suptitle("Features Distributions")

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

"""At the first glance while looking at the first graph that represents the “danceability” of the songs, we thought that the means were the same before and after the year of 2000. However, after calculating the p-value we realized that we were wrong and the p-value was smaller than 0.05. Since p-value was close to 0 we became able to understand that rejecting null hypothesis was the right decision. Besides, according to the graphs of the second row, which represented the popularity of the songs before and after the year of 2000 and their comparison, it is possible to claim that popularity of the pop songs has increased after 2000. This situation demonstrates that popularity of the pop songs are affected by the year changes. In the last row, we see the histogram of acousticness levels of the pop songs in the timeline of pre-2000’s and post-2000’s. By that graphs, one can state that the feature of acousticness has decreased as the time passes. This situation, which is the decrease of the acousticness of songs, is also seen in the comparison graph.

## Machine Learning
In this step , our purposes were using machine learning algorithms in order to guess the popularity of the songs according to the features of the songs, and to make a simple Spotify song recommendation system that takes song names and artist names as inputs from the user and recommends songs to the user according to his/her music taste. To do so, at first we cleaned and checked our data to be sure that no irrelavent (i.e ,id number of the songs) and invalid data was existing in our dataset. Then, we did label encoding for the features which were important for popularity guessing but were not in the right form. After that, we used the algorithms of linear regression, random forest, and decision tree in order to observe the difference between the real and the predicted popularity values. We analyzed the error rates and how they change in different algorithms that were explained in the previous sentence. Besides, we observed changing hypermeter impact on the error. In the last part, we made a song recommendation system that takes name of the songs and the artists as inputs and gives similar songs back to the users according to the features of the songs that were given as input at the first place.

### Preprocessing for Machine Learning
In this preprocessing for machine learning part, firstly , we copied our data that exist in df_main to a new dataframe that is called as ml_data . Following that , we have analyzed this dataframe by demonstrating column, row numbers and data amounts, and the validity of the values in the columns, rows.
"""

ml_data = df_main.copy() #create a new dataframe for machine learning
ml_data.head()

ml_data.size

ml_data.shape

ml_data.columns

print("Is there null:")
for col in ml_data:
  check_for_nan = ml_data[col].isnull().values.any()
  print(col, " - ", check_for_nan)

ml_data.isna().sum()*100/ml_data.shape[0]

ml_data.info()

"""We do not have any invalid values in the columns. So we didn't need to fix or drop any columns related to this issue.

However, we dropped the columns which are not numeric or not related to popularity (like *key* and *id*).
"""

ml_data.head()

"""######Label Encoding
In our dataset , the name of the artists and the genres of the songs were given to us in a form of strings. However, in order to use those informations in the upcoming parts we had to transform them into numerical values. Thus, we made new columns that have the numerical transformations of the values in the columns of artists and genres.
"""

from sklearn.preprocessing import OrdinalEncoder
encoder = OrdinalEncoder()

ml_data["artists_encoded"] = encoder.fit_transform(ml_data[["artists"]])
ml_data.head()

ml_data["genres_encoded"] = encoder.fit_transform(ml_data[["genres"]])
ml_data.head()

"""######Cleaning Data
Since in the next step we were going to use regression to guess popularity, we had to get rid of columns that were irrelevant to our purpose such as "key" and "id" . Besides, because of the fact that we encoded the columns of genres and artists we removed the string version columns of them from our dataframe.
"""

ml_data = ml_data.drop(["id","key","artist","artists","genres","name", "release_date", "explicit", "mode"], axis=1)
ml_data.head()

"""### Machine Learning For Popularity Prediction
In order to make a prediction of the popularity of the songs according to their features such as danceability,liveness, and so on , we used algorithms of linear regression, random forest, and decision tree. We compared the real values of the popularities and the predicted values that are obtained by those machine learning algorithms. Later on, we made a comparison of the errors that occurred by those algorithms and chose the most proper algorithm by looking the minimum error.

#### Splitting The Dataset For Training, Validation And Test
In the splitting the dataset part, we spared our machine learning data into three parts that are train , test , and validation. First, we took test size as 0.2 and the remainder 0.8 value was splitted into validation and train set also in the size of 0.2 . Later on, we removed "popularity" column from our x's (x_train , x_test , x_val) since it was the feature to be guessed and made y's (y_train , y_val , y_test) that contain only the column of popularity.
"""

#Split the dataset
from sklearn.model_selection import train_test_split

#train and test
train, test = train_test_split(ml_data, test_size=0.2, random_state=42)

#train and validation
train, val = train_test_split(train, test_size=0.2, random_state=42)


X_train = train.drop(["popularity"],axis=1)
Y_train = train["popularity"]

X_val = val.drop(["popularity"],axis=1)
Y_val = val["popularity"]

X_test = test.drop(["popularity"],axis=1)
Y_test = test["popularity"]

#X_train, X_test, Y_train, Y_test = train_test_split(ml_data, ml_data["popularity"], test_size=0.2, random_state=42)

print("Size of X_train:", X_train.size)
print("Size of X_test:" ,X_test.size)

X_train.head()

"""#### Linear Regression"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression
from sklearn import metrics
from sklearn.metrics import mean_squared_error, mean_absolute_error
# %matplotlib inline

"""##### Popularity vs Danceability"""

l1 = ml_data["danceability"].values.reshape(-1,1)
y1 = ml_data["popularity"].values.reshape(-1,1)

l1_train, l1_test, y1_train, y1_test = train_test_split(l1, y1, random_state=42, test_size=0.2)

regr1 = linear_model.LinearRegression()
regr1.fit(l1_train, y1_train)

y1_pred = regr1.predict(l1_test)
y1_pred

"""The table below, demonstrates the real values of popularities in the dataframe and the predicted values of the popularity that were guessed by the algorithm of linear regression according to the feature of danceability of the songs."""

df1 = pd.DataFrame({'Actual': y1_test.flatten(), 'Predicted': y1_pred.flatten()})
df1

"""This graph is the visualization of the data that are shown in the above table."""

plt.scatter(l1_test, y1_test,  color='teal')
plt.plot(l1_test, y1_pred, color='red', linewidth=3)
plt.xticks(())
plt.yticks(())
plt.show()

# Commented out IPython magic to ensure Python compatibility.
print('Coefficients: \n', regr1.coef_)
# The mean squared error
print('Mean squared error: %.2f'
#       % mean_squared_error(y1_test, y1_pred))
# The coefficient of determination: 1 is perfect prediction
print('Coefficient of determination: %.2f'
#       % r2_score(y1_test, y1_pred))

"""The coefficient of determination that is given to us by that algorithm is 0.01 which is far away from the value of 1. Thus, the linear regression popularity prediction by danceability did not provide us a good prediction.

##### Popularity vs Liveness
"""

l2 = ml_data["liveness"].values.reshape(-1,1)
y2 = ml_data["popularity"].values.reshape(-1,1)

l2_train, l2_test, y2_train, y2_test = train_test_split(l2, y2, random_state=42, test_size=0.2)

regr2 = linear_model.LinearRegression()
regr2.fit(l2_train, y2_train)

y2_pred = regr2.predict(l2_test)
y2_pred

"""The table below, demonstrates the real values of popularities in the dataframe and the predicted values of the popularity that were guessed by the algorithm of linear regression according to the feature of liveness."""

df2 = pd.DataFrame({'Actual': y2_test.flatten(), 'Predicted': y2_pred.flatten()})
df2

"""This graph is the visualization of the data that are shown in the above table."""

plt.scatter(l2_test, y2_test,  color='teal')
plt.plot(l2_test, y2_pred, color='red', linewidth=3)
plt.xticks(())
plt.yticks(())
plt.show()

# Commented out IPython magic to ensure Python compatibility.
print('Coefficients: \n', regr2.coef_)
# The mean squared error
print('Mean squared error: %.2f'
#       % mean_squared_error(y2_test, y2_pred))
# The coefficient of determination: 1 is perfect prediction
print('Coefficient of determination: %.2f'
#       % r2_score(y1_test, y2_pred))

"""#####Popularity vs Acousticness"""

l3 = ml_data["acousticness"].values.reshape(-1,1)
y3 = ml_data["popularity"].values.reshape(-1,1)

l3_train, l3_test, y3_train, y3_test = train_test_split(l3, y3, random_state=42, test_size=0.2)

regr3 = linear_model.LinearRegression()
regr3.fit(l3_train, y3_train)

y3_pred = regr3.predict(l3_test)
y3_pred

"""The table below, demonstrates the real values of popularities in the dataframe and the predicted values of the popularity that were guessed by the algorithm of linear regression according to the feature of acousticness."""

df3 = pd.DataFrame({'Actual': y3_test.flatten(), 'Predicted': y3_pred.flatten()})
df3

"""This graph is the visualization of the data that are shown in the above table."""

plt.scatter(l3_test, y3_test,  color='teal')
plt.plot(l3_test, y3_pred, color='red', linewidth=3)
plt.xticks(())
plt.yticks(())
plt.show()

# Commented out IPython magic to ensure Python compatibility.
print('Coefficients: \n', regr3.coef_)
# The mean squared error
print('Mean squared error: %.2f'
#       % mean_squared_error(y3_test, y3_pred))
# The coefficient of determination: 1 is perfect prediction
print('Coefficient of determination: %.2f'
#       % r2_score(y3_test, y3_pred))

"""By that, we got the coefficient of determination as 0,16. Although, it is still away from 1 , we can claim that the prediction is more reliable compared to the coefficient of determination value we got in the popularity vs danceability part. Thus, it wouldn't be wrong to say linear regression algorithm makes more correct predictions when the feature that is evaluated is acousticness not danceability.

#### Multiple Linear Regression
"""

X_train_mlr, X_test_mlr, y_train_mlr, y_test_mlr = train_test_split(ml_data.drop(["popularity"], axis=1), ml_data["popularity"], test_size=0.2, random_state=42)

regressor = LinearRegression()  
regressor.fit(X_train_mlr, y_train_mlr)

LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)

coeff_df = pd.DataFrame(regressor.coef_, ml_data.drop(["popularity"], axis=1).columns, columns=['Coefficient'])  
coeff_df

mlr_predicted = regressor.predict(X_train_mlr)

print('Mean Absolute Error of Multiple Linear Regression for training:', metrics.mean_absolute_error(y_train_mlr, mlr_predicted))
print('Mean Squared Error of Multiple Linear Regression for training:', metrics.mean_squared_error(y_train_mlr, mlr_predicted))
print('Root Mean Squared Error of Multiple Linear Regression for training:', np.sqrt(metrics.mean_squared_error(y_train_mlr, mlr_predicted)))
print('R-square score of Multiple Linear Regression for training: ', metrics.r2_score(y_train_mlr, mlr_predicted))

mlr_predicted_test = regressor.predict(X_test_mlr)

print('Mean Absolute Error of Multiple Linear Regression:', metrics.mean_absolute_error(y_test_mlr, mlr_predicted_test))
print('Mean Squared Error of Multiple Linear Regression:', metrics.mean_squared_error(y_test_mlr, mlr_predicted_test))
print('Root Mean Squared Error of Multiple Linear Regression:', np.sqrt(metrics.mean_squared_error(y_test_mlr, mlr_predicted_test)))
print('R-square score of Multiple Linear Regression: ', metrics.r2_score(y_test_mlr, mlr_predicted_test))

"""In the above, we got the mean absolute error of 13.28. Since this is the first machine learning algorithm we used, it is early to make a comment. After, using other algorithms we will compare the mean absolute error and decide which one is better to use.

The table below, demonstrates the real values of popularities in the dataframe and the predicted values of the popularity that were guessed by the algorithm of linear regression according to the all features of the songs.
"""

df4 = pd.DataFrame({'Actual': y_test_mlr, 'Predicted': mlr_predicted_test})
df5 = df4.head(15)
df5

"""This bar chart is the visualization of the predicted values and the real values. By looking at that, we can say that most of the time the linear regression algorithm prediction was more than the real popularity value."""

df5.plot(kind='bar',figsize=(10,8), colormap="cool")
plt.grid(axis="y", linestyle='-', linewidth='0.5', color='black')
plt.show()

"""#### Random Forest"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split

#Random Forest Training
model_rf = RandomForestRegressor(random_state=42, max_depth=5)
model_rf.fit(X_train, Y_train)

model_rf.feature_importances_

"""This bar chart provides us the information of the importance ranking of the features while guessing the popularity. By looking at that chart, it can be claimed that the year, genres, instrumentalness features of the songs are important for popularity guessing. Besides, we can see that the features mode, liveness, tempo and explicit are not much related with the popularity of the song. We can try to improve our model by dropping these columns."""

feature_importances = pd.Series(model_rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)
plt.figure(figsize=(11, 7))
sns.barplot(x=feature_importances, y=feature_importances.index)

# Add labels to our graph  
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.title("Feature Importance Rankings")
plt.show()

#Random Forest Testing (with validation set)
rf_predictions = model_rf.predict(X_val)

mse = mean_squared_error(Y_val, rf_predictions)
mae = mean_absolute_error(Y_val, rf_predictions)
rmse = np.sqrt(mse)

print("mse: {}".format(mse))
print("mae: {}".format(mae))
print("rmse: {}".format(rmse))

"""The mean absolute error we get by random forest algorithm is less than we got in linear regression, which means that this algorithm is better for the prediction of popularity."""

mse = []
mae = []
rmse = []

depth_range = range(4,24,2)

for i in depth_range:
  model_rf = RandomForestRegressor(n_estimators=50, random_state=42, max_depth=i)
  model_rf.fit(X_train, Y_train)

  #Random Forest Testing (with validation set)
  rf_predictions = model_rf.predict(X_val)

  mse.append(mean_squared_error(Y_val, rf_predictions))
  mae.append(mean_absolute_error(Y_val, rf_predictions))
  rmse.append(np.sqrt(mse[-1]))

plt.figure(figsize=(10,5))
plt.subplot(1, 2, 1)
plt.plot(depth_range, mae, color="darkslategray", label="mae")
plt.plot(depth_range, rmse, color = "lightslategray", label="rmse")
plt.ylabel("MAE and RMSE")
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(depth_range, mse, color="teal", label="mse")
plt.xlabel("max_depth")
plt.ylabel("MSE")
plt.legend()

plt.tight_layout(pad=5)
plt.show()

"""This graph demonstrates the importance of max_depth in the amount of errors. As it is seen in this graph, increasing max_depth decreases the errors , especially mean squared error.

#### Decision Tree
"""

from sklearn.tree import DecisionTreeRegressor
from sklearn import datasets
from sklearn.tree import DecisionTreeRegressor
from sklearn import tree
regressor = DecisionTreeRegressor(random_state = 42, max_depth=15)
regressor.fit(X_train, Y_train)

y_pred = regressor.predict(X_test)

"""The table below, demonstrates the real values of popularities in the dataframe and the predicted values of the popularity that were guessed by the algorithm of decision tree according to the features."""

da =pd.DataFrame({'Actual':Y_test, 'Predicted':y_pred})
da

from sklearn import metrics
print('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, y_pred))
print('Mean Squared Error:', metrics.mean_squared_error(Y_test, y_pred))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, y_pred)))

"""As a conclusion, in decision tree algorithm we get a mean absolute error that is less than the ones we got in the linear regression and random forest algorithm, which means that it is more reliable compared to them in popularity guessing.

This is the representation of the decision tree in the form of text.
"""

regr = DecisionTreeRegressor(max_depth=4, random_state=1234)
model = regr.fit(X_train, Y_train)
text_representation = tree.export_text(regr)
print(text_representation)

"""Furthermore, it is possible to visualize our decision tree. The color of the leaf corresponds to the predicted value."""

fig = plt.figure(figsize=(28,10), facecolor = "white" , edgecolor="orange")
a= tree.plot_tree(regr, feature_names= X_train.columns, filled=True, fontsize=10)

"""Being different from our first decision tree algorithm example, this time as extra we will drop some of the attributes from our training dataset. 

"""

X_removed = X_train.drop(["acousticness", "year", "energy"] , axis=1)
Y = Y_train
x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(X_removed, Y, test_size=0.2, random_state=0)
regressor = DecisionTreeRegressor(random_state = 42)
regressor.fit(x_train_2, y_train_2)
y_pred_2 = regressor.predict(x_test_2)
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test_2, y_pred_2))
print('Mean Squared Error:', metrics.mean_squared_error(y_test_2, y_pred_2))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test_2, y_pred_2)))

"""The mean absolute error was increased after we removed these attributes. This situation shows that these features are for our decision tree model important while guessing the popularity.

The following graph demonstrates the importance of max_depth in the amount of errors.
"""

mse = []
mae = []
rmse = []

depth_range = range(4,20,2)

for i in depth_range:
  model_dt = DecisionTreeRegressor(random_state=42, max_depth=i)
  model_dt.fit(X_train, Y_train)

  #Random Forest Testing (with validation set)
  rf_predictions = model_dt.predict(X_val)

  mse.append(mean_squared_error(Y_val, rf_predictions))
  mae.append(mean_absolute_error(Y_val, rf_predictions))
  rmse.append(np.sqrt(mse[-1]))

plt.figure(figsize=(10,5))
plt.subplot(1, 2, 1)
plt.plot(depth_range, mae, color="darkslategray", label="mae")
plt.plot(depth_range, rmse, color = "lightslategray", label="rmse")
plt.ylabel("MAE and RMSE")
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(depth_range, mse, color="teal", label="mse")
plt.xlabel("max_depth")
plt.ylabel("MSE")
plt.legend()

plt.tight_layout(pad=5)
plt.show()

"""### Machine Learning for Song Recommendation

In this part, our purpose was to make a recommendation system that takes the name of the artists and the songs from the user and recommends similar songs to the user according to the values of the features of the songs that the user gave as input. To do so, we used nearest neighbor algorithm and chose the songs that are similar to the input song in the content of features.
"""

import numpy as np
from sklearn.neighbors import NearestNeighbors

key_arr = []
val_arr = []

n = 5

for i in range(n):
  k = input("Enter artist name: ")
  v = input("Enter song name: ")
  print()
  key_arr.append(k)
  val_arr.append(v)

print(key_arr)
print(val_arr)

fav_songs_df = pd.DataFrame(columns=df_main.columns)

for i in range(n):
  song = df_main.loc[(df_main['name'] == val_arr[i]) & (df_main['artist'] == key_arr[i])]
  song = song.head(1)
  fav_songs_df = fav_songs_df.append(song)

fav_songs_df

fav_songs_df = fav_songs_df.drop(["artists", "id", "key", "mode", "release_date","popularity","duration_ms", "name", "artist", "genres"], axis=1)

fav_songs_df

feature_means = [[]]  
for i in fav_songs_df:
  feature_means[0].append(fav_songs_df[i].mean())
feature_means[0]

from sklearn.neighbors import NearestNeighbors

samples = df_main
samples = samples.drop(["artists", "id", "key", "mode", "release_date", "name", "popularity","duration_ms","artist", "genres"], axis=1)

neigh = NearestNeighbors(n_neighbors=5)
neigh.fit(samples)

results = neigh.kneighbors(feature_means)
results

for result in results[1][0]:
  res = df_main.loc[result]
  print(res["name"], "-", res["artist"])

"""##Conclusion

To conclude, at first we loaded our data models into our notebook. Only loading the data was not enough and we also needed to check if the dataset is consistent and values are valid. After checking the values, we saw that there was no mistake but to get a much more larger scale of key and value pairs then we manipulated our dataset to be able to reach genres. Since we wanted to understand and observe our data, we analyzed our data in various ways. We used plots, scatters, histograms and line graphs to learn and get used to our data.

Secondly, we came up with two hypotheses. The first hypothesis we selected was that "Popularity is same for all modes." and the second hypothesis was "Before and after 2000's the features Danceability, Popularity and Acousticness doesn't change.". After conducting experiments and ploting graphs with their p_values we came up to a result that the null hypothesis were not correct. So, we next claimed that alternative hypothesis were correct. These alternative hypothesis were "Popularity is not same for all modes" and "Before and after 2000's the features Danceability, Popularity and Acousticness changed.".

Lastly, in the machine learning step, we had two purposes that were predicting the popularity of the songs according to their features and create a simple song recommendation system. We used several machine learning algorithms which were linear regression, decision tree, and random forest in order to have same guess, if not a close guess to the real popularity value. We have observed that none of those machine learning algorithms gave us values that were really close to the real value. However, we saw the importance of hyperparameter tuning. As an example of that, we changed the value of max_depth for machine learning algorithms and as a result of this we got more close results that was understood by the decrease in the errors. Besides, after comparing those three machine learning algorithms we saw that decision tree algorithm was a better machine learning algorithm for our dataset and our purpose. For the second part of the task, we took song names and artist names as input from the user and by evaluating the features means of the songs that exist in our dataset we gave the user the most similar songs to the previously given input.
"""

